{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf548b1a",
   "metadata": {},
   "source": [
    "# Part 2: Fine-tune GPT-2 with Emotion Conditioning\n",
    "\n",
    "This notebook implements **four conditioning methods** for emotion-controlled text generation:\n",
    "\n",
    "- **Method A (Baseline):** Plain GPT-2 without explicit emotion conditioning\n",
    "- **Method B (Prefix):** Text prefix conditioning (e.g., \"happy: ...\")\n",
    "- **Method C (Token):** Special emotion tokens (e.g., `<HAPPY>`)\n",
    "- **Method D (LoRA):** Parameter-efficient fine-tuning with LoRA + special tokens\n",
    "\n",
    "## References\n",
    "- Hu et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models. https://arxiv.org/abs/2106.09685\n",
    "- Singh et al. (2020). Controlled Affective Text Generation. https://arxiv.org/abs/2011.04000\n",
    "- Resendiz & Klinger (2023). Emotion-Conditioned Text Generation. https://aclanthology.org/2023.tllm-1.3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cd62917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\LLM_Chatbot\\llmvenv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "from utils.emotion_mapping import EMOTION_TOKENS, TARGET_EMOTIONS\n",
    "from utils.dailydialog_processor import load_and_prepare_dailydialog, get_emotion_distribution\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959bf15a",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare DailyDialog Dataset\n",
    "\n",
    "We'll prepare the dataset for all four methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16152b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DailyDialog to inspect structure...\n",
      "\n",
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "\n",
      "First sample from train:\n",
      "Available fields: dict_keys(['id', 'acts', 'emotions', 'utterances'])\n",
      "\n",
      "Sample content:\n",
      "  id: a438b751ab9997cdb35f07bfe3dfb010b96365f4762d77f87e5f41290ff61c3d_0\n",
      "  acts: [3, 4, 2, 2, 2, 3, 4, 1, 3, 4]\n",
      "  emotions: [0, 0, 0, 0, 0, 0, 4, 4, 4, 4]\n",
      "  utterances: ['Say , Jim , how about going for a few beers after dinner ?', 'You know that is tempting but is really not good for our fitness .', 'What do you mean ? It will help us to relax .', \"Do you really thi...\n",
      "\n",
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "\n",
      "First sample from train:\n",
      "Available fields: dict_keys(['id', 'acts', 'emotions', 'utterances'])\n",
      "\n",
      "Sample content:\n",
      "  id: a438b751ab9997cdb35f07bfe3dfb010b96365f4762d77f87e5f41290ff61c3d_0\n",
      "  acts: [3, 4, 2, 2, 2, 3, 4, 1, 3, 4]\n",
      "  emotions: [0, 0, 0, 0, 0, 0, 4, 4, 4, 4]\n",
      "  utterances: ['Say , Jim , how about going for a few beers after dinner ?', 'You know that is tempting but is really not good for our fitness .', 'What do you mean ? It will help us to relax .', \"Do you really thi...\n"
     ]
    }
   ],
   "source": [
    "# First, let's check the actual structure of DailyDialog dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading DailyDialog to inspect structure...\")\n",
    "dd_dataset = load_dataset(\"roskoN/dailydialog\")\n",
    "\n",
    "print(f\"\\nDataset splits: {dd_dataset.keys()}\")\n",
    "print(f\"\\nFirst sample from train:\")\n",
    "sample = dd_dataset['train'][0]\n",
    "print(f\"Available fields: {sample.keys()}\")\n",
    "print(f\"\\nSample content:\")\n",
    "for key, value in sample.items():\n",
    "    print(f\"  {key}: {value if len(str(value)) < 200 else str(value)[:200] + '...'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DailyDialog dataset for all methods...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Preparing data for: BASELINE\n",
      "============================================================\n",
      "Loading DailyDialog dataset...\n",
      "Extracting context-response pairs...\n",
      "Extracting context-response pairs...\n",
      "Train samples: 76052\n",
      "Validation samples: 7069\n",
      "Test samples: 6740\n",
      "Preparing data for method: baseline\n",
      "\n",
      "=== Sample from baseline dataset ===\n",
      "Say , Jim , how about going for a few beers after dinner ?\n",
      "You know that is tempting but is really not good for our fitness .\n",
      "Emotion: neutral\n",
      "\n",
      "Emotion distribution:\n",
      "  neutral: 62357 (82.0%)\n",
      "  happy: 10253 (13.5%)\n",
      "  surprise: 1504 (2.0%)\n",
      "\n",
      "============================================================\n",
      "Preparing data for: PREFIX\n",
      "============================================================\n",
      "Loading DailyDialog dataset...\n",
      "Train samples: 76052\n",
      "Validation samples: 7069\n",
      "Test samples: 6740\n",
      "Preparing data for method: baseline\n",
      "\n",
      "=== Sample from baseline dataset ===\n",
      "Say , Jim , how about going for a few beers after dinner ?\n",
      "You know that is tempting but is really not good for our fitness .\n",
      "Emotion: neutral\n",
      "\n",
      "Emotion distribution:\n",
      "  neutral: 62357 (82.0%)\n",
      "  happy: 10253 (13.5%)\n",
      "  surprise: 1504 (2.0%)\n",
      "\n",
      "============================================================\n",
      "Preparing data for: PREFIX\n",
      "============================================================\n",
      "Loading DailyDialog dataset...\n",
      "Extracting context-response pairs...\n",
      "Extracting context-response pairs...\n",
      "Train samples: 76052\n",
      "Validation samples: 7069\n",
      "Test samples: 6740\n",
      "Preparing data for method: prefix\n",
      "\n",
      "=== Sample from prefix dataset ===\n",
      "Say , Jim , how about going for a few beers after dinner ?\n",
      "neutral: You know that is tempting but is really not good for our fitness .\n",
      "Emotion: neutral\n",
      "\n",
      "Emotion distribution:\n",
      "  neutral: 62357 (82.0%)\n",
      "  happy: 10253 (13.5%)\n",
      "  surprise: 1504 (2.0%)\n",
      "\n",
      "============================================================\n",
      "Preparing data for: TOKEN\n",
      "============================================================\n",
      "Loading DailyDialog dataset...\n",
      "Train samples: 76052\n",
      "Validation samples: 7069\n",
      "Test samples: 6740\n",
      "Preparing data for method: prefix\n",
      "\n",
      "=== Sample from prefix dataset ===\n",
      "Say , Jim , how about going for a few beers after dinner ?\n",
      "neutral: You know that is tempting but is really not good for our fitness .\n",
      "Emotion: neutral\n",
      "\n",
      "Emotion distribution:\n",
      "  neutral: 62357 (82.0%)\n",
      "  happy: 10253 (13.5%)\n",
      "  surprise: 1504 (2.0%)\n",
      "\n",
      "============================================================\n",
      "Preparing data for: TOKEN\n",
      "============================================================\n",
      "Loading DailyDialog dataset...\n",
      "Extracting context-response pairs...\n",
      "Extracting context-response pairs...\n",
      "Train samples: 76052\n",
      "Validation samples: 7069\n",
      "Test samples: 6740\n",
      "Preparing data for method: token\n",
      "\n",
      "=== Sample from token dataset ===\n",
      "<NEUTRAL> Say , Jim , how about going for a few beers after dinner ?\n",
      "You know that is tempting but is really not good for our fitness .\n",
      "Emotion: neutral\n",
      "\n",
      "Emotion distribution:\n",
      "  neutral: 62357 (82.0%)\n",
      "  happy: 10253 (13.5%)\n",
      "  surprise: 1504 (2.0%)\n",
      "\n",
      "============================================================\n",
      "Preparing data for: LORA\n",
      "============================================================\n",
      "Loading DailyDialog dataset...\n",
      "Train samples: 76052\n",
      "Validation samples: 7069\n",
      "Test samples: 6740\n",
      "Preparing data for method: token\n",
      "\n",
      "=== Sample from token dataset ===\n",
      "<NEUTRAL> Say , Jim , how about going for a few beers after dinner ?\n",
      "You know that is tempting but is really not good for our fitness .\n",
      "Emotion: neutral\n",
      "\n",
      "Emotion distribution:\n",
      "  neutral: 62357 (82.0%)\n",
      "  happy: 10253 (13.5%)\n",
      "  surprise: 1504 (2.0%)\n",
      "\n",
      "============================================================\n",
      "Preparing data for: LORA\n",
      "============================================================\n",
      "Loading DailyDialog dataset...\n",
      "Extracting context-response pairs...\n",
      "Extracting context-response pairs...\n",
      "Train samples: 76052\n",
      "Validation samples: 7069\n",
      "Test samples: 6740\n",
      "Preparing data for method: token\n",
      "\n",
      "=== Sample from token dataset ===\n",
      "<NEUTRAL> Say , Jim , how about going for a few beers after dinner ?\n",
      "You know that is tempting but is really not good for our fitness .\n",
      "Emotion: neutral\n",
      "\n",
      "Emotion distribution:\n",
      "  neutral: 62357 (82.0%)\n",
      "  happy: 10253 (13.5%)\n",
      "  surprise: 1504 (2.0%)\n",
      "\n",
      "âœ… All datasets prepared successfully!\n",
      "Train samples: 76052\n",
      "Validation samples: 7069\n",
      "Test samples: 6740\n",
      "Preparing data for method: token\n",
      "\n",
      "=== Sample from token dataset ===\n",
      "<NEUTRAL> Say , Jim , how about going for a few beers after dinner ?\n",
      "You know that is tempting but is really not good for our fitness .\n",
      "Emotion: neutral\n",
      "\n",
      "Emotion distribution:\n",
      "  neutral: 62357 (82.0%)\n",
      "  happy: 10253 (13.5%)\n",
      "  surprise: 1504 (2.0%)\n",
      "\n",
      "âœ… All datasets prepared successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "METHODS = ['baseline', 'prefix', 'token', 'lora']\n",
    "MODEL_NAME = 'microsoft/DialoGPT-small'  # Better dialogue-pretrained model\n",
    "USE_CONTEXT = True  # Use previous utterance as context\n",
    "\n",
    "print(\"Loading DailyDialog dataset for all methods...\\n\")\n",
    "\n",
    "# Load data for each method\n",
    "datasets = {}\n",
    "for method in METHODS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Preparing data for: {method.upper()}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    data = load_and_prepare_dailydialog(\n",
    "        method=method if method != 'lora' else 'token',  # LoRA uses same format as token\n",
    "        use_context=USE_CONTEXT\n",
    "    )\n",
    "    datasets[method] = data\n",
    "    \n",
    "    # Show emotion distribution\n",
    "    dist = get_emotion_distribution(data['train'])\n",
    "    print(f\"\\nEmotion distribution:\")\n",
    "    for emotion, count in sorted(dist.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "        print(f\"  {emotion}: {count} ({count/len(data['train'])*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nâœ… All datasets prepared successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79baa38",
   "metadata": {},
   "source": [
    "## 2. Modular Training Function\n",
    "\n",
    "This function handles training for all four methods with appropriate configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90923aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def prepare_tokenizer(method, base_model_name='microsoft/DialoGPT-small'):\n",
    "    \"\"\"\n",
    "    Prepare tokenizer with special tokens if needed.\n",
    "    \n",
    "    Args:\n",
    "        method (str): Training method\n",
    "        base_model_name (str): Base model name\n",
    "        \n",
    "    Returns:\n",
    "        AutoTokenizer: Configured tokenizer\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    \n",
    "    # Set pad token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Add special emotion tokens for token and LoRA methods\n",
    "    if method in ['token', 'lora']:\n",
    "        num_added = tokenizer.add_special_tokens({'additional_special_tokens': EMOTION_TOKENS})\n",
    "        print(f\"  Added {num_added} emotion tokens: {EMOTION_TOKENS}\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def prepare_model(method, tokenizer, base_model_name='microsoft/DialoGPT-small'):\n",
    "    \"\"\"\n",
    "    Prepare model with LoRA if needed.\n",
    "    \n",
    "    Args:\n",
    "        method (str): Training method\n",
    "        tokenizer: Tokenizer (for resizing embeddings)\n",
    "        base_model_name (str): Base model name\n",
    "        \n",
    "    Returns:\n",
    "        Model: Configured model\n",
    "    \"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "    \n",
    "    # Resize embeddings if tokens were added\n",
    "    if method in ['token', 'lora']:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        print(f\"  Resized embeddings to {len(tokenizer)} tokens\")\n",
    "    \n",
    "    # Apply LoRA for method D\n",
    "    if method == 'lora':\n",
    "        print(\"  Applying LoRA configuration...\")\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            r=8,  # Rank\n",
    "            lora_alpha=16,  # Alpha\n",
    "            lora_dropout=0.05,\n",
    "            target_modules=['c_attn', 'c_proj'],  # GPT-2 attention modules\n",
    "            bias=\"none\"\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def tokenize_dataset(data, tokenizer, max_length=256):\n",
    "    \"\"\"\n",
    "    Tokenize dataset for language modeling.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of samples with 'text' field\n",
    "        tokenizer: Tokenizer\n",
    "        max_length (int): Maximum sequence length\n",
    "        \n",
    "    Returns:\n",
    "        Dataset: Tokenized dataset\n",
    "    \"\"\"\n",
    "    texts = [sample['text'] for sample in data]\n",
    "    \n",
    "    # Tokenize\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False  # Dynamic padding in collator\n",
    "    )\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_dict({\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': encodings['attention_mask']\n",
    "    })\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def train_gpt2_variant(\n",
    "    method,\n",
    "    train_data,\n",
    "    val_data,\n",
    "    base_model_name='microsoft/DialoGPT-small',\n",
    "    num_epochs=5,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    max_length=256,\n",
    "    output_dir=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train GPT-2 variant with specified conditioning method.\n",
    "    \n",
    "    Args:\n",
    "        method (str): Training method ('baseline', 'prefix', 'token', 'lora')\n",
    "        train_data (list): Training data\n",
    "        val_data (list): Validation data\n",
    "        base_model_name (str): Base model name\n",
    "        num_epochs (int): Number of training epochs\n",
    "        batch_size (int): Training batch size\n",
    "        learning_rate (float): Learning rate\n",
    "        max_length (int): Maximum sequence length\n",
    "        output_dir (str): Output directory for model\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trainer, tokenizer, model)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training Method: {method.upper()}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    # Prepare tokenizer and model\n",
    "    print(\"\\n1. Preparing tokenizer...\")\n",
    "    tokenizer = prepare_tokenizer(method, base_model_name)\n",
    "    \n",
    "    print(\"\\n2. Preparing model...\")\n",
    "    model = prepare_model(method, tokenizer, base_model_name)\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    print(\"\\n3. Tokenizing datasets...\")\n",
    "    train_dataset = tokenize_dataset(train_data, tokenizer, max_length)\n",
    "    val_dataset = tokenize_dataset(val_data, tokenizer, max_length)\n",
    "    print(f\"  Train samples: {len(train_dataset)}\")\n",
    "    print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False  # Causal LM, not masked LM\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    if output_dir is None:\n",
    "        output_dir = f\"../models/gpt2_{method}\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=1000,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=500,\n",
    "        evaluation_strategy=\"epoch\",  # FIXED: Changed from \"steps\" to \"epoch\" to match save_strategy\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "        seed=42,\n",
    "        fp16=torch.cuda.is_available()  # Use mixed precision if GPU available\n",
    "    )\n",
    "    \n",
    "    print(\"\\n4. Training configuration:\")\n",
    "    print(f\"  Epochs: {num_epochs}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Learning rate: {learning_rate}\")\n",
    "    print(f\"  Max length: {max_length}\")\n",
    "    print(f\"  Output: {output_dir}\")\n",
    "    print(f\"  FP16: {training_args.fp16}\")\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    print(\"\\n5. Starting training...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Train\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training complete!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Display training metrics\n",
    "    print(\"\\n=== Training Metrics ===\")\n",
    "    for key, value in train_result.metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # Save model and tokenizer\n",
    "    print(f\"\\n6. Saving model to {output_dir}...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # For LoRA, also save the base model configuration\n",
    "    if method == 'lora':\n",
    "        model.save_pretrained(output_dir)\n",
    "    \n",
    "    print(\"\\nâœ… Model saved successfully!\\n\")\n",
    "    \n",
    "    return trainer, tokenizer, model\n",
    "\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375aaca",
   "metadata": {},
   "source": [
    "## 3. Train Method A: Baseline\n",
    "\n",
    "Plain GPT-2 without explicit emotion conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f300db4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training Method: BASELINE\n",
      "======================================================================\n",
      "\n",
      "1. Preparing tokenizer...\n",
      "\n",
      "2. Preparing model...\n",
      "\n",
      "2. Preparing model...\n",
      "\n",
      "3. Tokenizing datasets...\n",
      "\n",
      "3. Tokenizing datasets...\n",
      "  Train samples: 76052\n",
      "  Validation samples: 7069\n",
      "\n",
      "4. Training configuration:\n",
      "  Epochs: 2\n",
      "  Batch size: 8\n",
      "  Learning rate: 5e-05\n",
      "  Max length: 256\n",
      "  Output: ../models/gpt2_baseline\n",
      "  FP16: True\n",
      "  Train samples: 76052\n",
      "  Validation samples: 7069\n",
      "\n",
      "4. Training configuration:\n",
      "  Epochs: 2\n",
      "  Batch size: 8\n",
      "  Learning rate: 5e-05\n",
      "  Max length: 256\n",
      "  Output: ../models/gpt2_baseline\n",
      "  FP16: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_Chatbot\\llmvenv\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_140100\\658104220.py:179: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Starting training...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19014' max='19014' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19014/19014 33:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.626200</td>\n",
       "      <td>2.713268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.388200</td>\n",
       "      <td>2.677005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training complete!\n",
      "======================================================================\n",
      "\n",
      "=== Training Metrics ===\n",
      "train_runtime: 2003.1452\n",
      "train_samples_per_second: 75.9330\n",
      "train_steps_per_second: 9.4920\n",
      "total_flos: 4617287414784000.0000\n",
      "train_loss: 2.6045\n",
      "epoch: 2.0000\n",
      "\n",
      "6. Saving model to ../models/gpt2_baseline...\n",
      "\n",
      "âœ… Model saved successfully!\n",
      "\n",
      "\n",
      "âœ… Model saved successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train baseline model\n",
    "trainer_baseline, tokenizer_baseline, model_baseline = train_gpt2_variant(\n",
    "    method='baseline',\n",
    "    train_data=datasets['baseline']['train'],\n",
    "    val_data=datasets['baseline']['validation'],\n",
    "    num_epochs=2,\n",
    "    batch_size=8,\n",
    "    learning_rate=5e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b216c8ff",
   "metadata": {},
   "source": [
    "## 4. Train Method B: Prefix Conditioning\n",
    "\n",
    "Text prefix conditioning (e.g., \"happy: response\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16cd291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training Method: PREFIX\n",
      "======================================================================\n",
      "\n",
      "1. Preparing tokenizer...\n",
      "\n",
      "2. Preparing model...\n",
      "\n",
      "2. Preparing model...\n",
      "\n",
      "3. Tokenizing datasets...\n",
      "\n",
      "3. Tokenizing datasets...\n",
      "  Train samples: 76052\n",
      "  Validation samples: 7069\n",
      "\n",
      "4. Training configuration:\n",
      "  Epochs: 2\n",
      "  Batch size: 8\n",
      "  Learning rate: 5e-05\n",
      "  Max length: 256\n",
      "  Output: ../models/gpt2_prefix\n",
      "  FP16: True\n",
      "  Train samples: 76052\n",
      "  Validation samples: 7069\n",
      "\n",
      "4. Training configuration:\n",
      "  Epochs: 2\n",
      "  Batch size: 8\n",
      "  Learning rate: 5e-05\n",
      "  Max length: 256\n",
      "  Output: ../models/gpt2_prefix\n",
      "  FP16: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_Chatbot\\llmvenv\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_140100\\658104220.py:179: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Starting training...\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19014' max='19014' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19014/19014 1:58:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.466400</td>\n",
       "      <td>2.534934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.251400</td>\n",
       "      <td>2.499738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training complete!\n",
      "======================================================================\n",
      "\n",
      "=== Training Metrics ===\n",
      "train_runtime: 7111.7001\n",
      "train_samples_per_second: 21.3880\n",
      "train_steps_per_second: 2.6740\n",
      "total_flos: 4775332349952000.0000\n",
      "train_loss: 2.4547\n",
      "epoch: 2.0000\n",
      "\n",
      "6. Saving model to ../models/gpt2_prefix...\n",
      "\n",
      "âœ… Model saved successfully!\n",
      "\n",
      "\n",
      "âœ… Model saved successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train prefix model with improved settings\n",
    "trainer_prefix, tokenizer_prefix, model_prefix = train_gpt2_variant(\n",
    "    method='prefix',\n",
    "    train_data=datasets['prefix']['train'],\n",
    "    val_data=datasets['prefix']['validation'],\n",
    "    num_epochs=5,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b20f98",
   "metadata": {},
   "source": [
    "## 5. Train Method C: Special Token Conditioning\n",
    "\n",
    "Special emotion tokens (e.g., `<HAPPY>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4068464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training Method: TOKEN\n",
      "======================================================================\n",
      "\n",
      "1. Preparing tokenizer...\n",
      "  Added 7 emotion tokens: ['<NEUTRAL>', '<HAPPY>', '<SAD>', '<ANGRY>', '<FEAR>', '<DISGUST>', '<SURPRISE>']\n",
      "\n",
      "2. Preparing model...\n",
      "  Added 7 emotion tokens: ['<NEUTRAL>', '<HAPPY>', '<SAD>', '<ANGRY>', '<FEAR>', '<DISGUST>', '<SURPRISE>']\n",
      "\n",
      "2. Preparing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Resized embeddings to 50264 tokens\n",
      "\n",
      "3. Tokenizing datasets...\n",
      "  Train samples: 76052\n",
      "  Validation samples: 7069\n",
      "\n",
      "4. Training configuration:\n",
      "  Epochs: 2\n",
      "  Batch size: 8\n",
      "  Learning rate: 5e-05\n",
      "  Max length: 256\n",
      "  Output: ../models/gpt2_token\n",
      "  FP16: True\n",
      "  Train samples: 76052\n",
      "  Validation samples: 7069\n",
      "\n",
      "4. Training configuration:\n",
      "  Epochs: 2\n",
      "  Batch size: 8\n",
      "  Learning rate: 5e-05\n",
      "  Max length: 256\n",
      "  Output: ../models/gpt2_token\n",
      "  FP16: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_Chatbot\\llmvenv\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_140100\\658104220.py:179: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Starting training...\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19014' max='19014' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19014/19014 52:28, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.653700</td>\n",
       "      <td>2.763767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.429500</td>\n",
       "      <td>2.729556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training complete!\n",
      "======================================================================\n",
      "\n",
      "=== Training Metrics ===\n",
      "train_runtime: 3148.6444\n",
      "train_samples_per_second: 48.3080\n",
      "train_steps_per_second: 6.0390\n",
      "total_flos: 4693380553728000.0000\n",
      "train_loss: 2.6360\n",
      "epoch: 2.0000\n",
      "\n",
      "6. Saving model to ../models/gpt2_token...\n",
      "\n",
      "âœ… Model saved successfully!\n",
      "\n",
      "\n",
      "âœ… Model saved successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train token model with improved settings\n",
    "trainer_token, tokenizer_token, model_token = train_gpt2_variant(\n",
    "    method='token',\n",
    "    train_data=datasets['token']['train'],\n",
    "    val_data=datasets['token']['validation'],\n",
    "    num_epochs=5,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223dc33b",
   "metadata": {},
   "source": [
    "## 6. Train Method D: LoRA with Token Conditioning\n",
    "\n",
    "Parameter-efficient fine-tuning with LoRA (Rank=8, Alpha=16, Dropout=0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e04a526a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training Method: LORA\n",
      "======================================================================\n",
      "\n",
      "1. Preparing tokenizer...\n",
      "  Added 7 emotion tokens: ['<NEUTRAL>', '<HAPPY>', '<SAD>', '<ANGRY>', '<FEAR>', '<DISGUST>', '<SURPRISE>']\n",
      "\n",
      "2. Preparing model...\n",
      "  Added 7 emotion tokens: ['<NEUTRAL>', '<HAPPY>', '<SAD>', '<ANGRY>', '<FEAR>', '<DISGUST>', '<SURPRISE>']\n",
      "\n",
      "2. Preparing model...\n",
      "  Resized embeddings to 50264 tokens\n",
      "  Applying LoRA configuration...\n",
      "trainable params: 811,008 || all params: 125,256,192 || trainable%: 0.6475\n",
      "\n",
      "3. Tokenizing datasets...\n",
      "  Resized embeddings to 50264 tokens\n",
      "  Applying LoRA configuration...\n",
      "trainable params: 811,008 || all params: 125,256,192 || trainable%: 0.6475\n",
      "\n",
      "3. Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_Chatbot\\llmvenv\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train samples: 76052\n",
      "  Validation samples: 7069\n",
      "\n",
      "4. Training configuration:\n",
      "  Epochs: 2\n",
      "  Batch size: 8\n",
      "  Learning rate: 0.0001\n",
      "  Max length: 256\n",
      "  Output: ../models/gpt2_lora\n",
      "  FP16: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_Chatbot\\llmvenv\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_140100\\658104220.py:179: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Starting training...\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19014' max='19014' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19014/19014 20:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.994300</td>\n",
       "      <td>2.993249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.962400</td>\n",
       "      <td>2.971061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_Chatbot\\llmvenv\\lib\\site-packages\\peft\\utils\\save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "d:\\LLM_Chatbot\\llmvenv\\lib\\site-packages\\peft\\utils\\save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "d:\\LLM_Chatbot\\llmvenv\\lib\\site-packages\\peft\\utils\\save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "d:\\LLM_Chatbot\\llmvenv\\lib\\site-packages\\peft\\utils\\save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "d:\\LLM_Chatbot\\llmvenv\\lib\\site-packages\\peft\\utils\\save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training complete!\n",
      "======================================================================\n",
      "\n",
      "=== Training Metrics ===\n",
      "train_runtime: 1208.4744\n",
      "train_samples_per_second: 125.8640\n",
      "train_steps_per_second: 15.7340\n",
      "total_flos: 4738131884335104.0000\n",
      "train_loss: 3.0214\n",
      "epoch: 2.0000\n",
      "\n",
      "6. Saving model to ../models/gpt2_lora...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_Chatbot\\llmvenv\\lib\\site-packages\\peft\\utils\\save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Model saved successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train LoRA model\n",
    "trainer_lora, tokenizer_lora, model_lora = train_gpt2_variant(\n",
    "    method='lora',\n",
    "    train_data=datasets['lora']['train'],\n",
    "    val_data=datasets['lora']['validation'],\n",
    "    num_epochs=2,\n",
    "    batch_size=8,\n",
    "    learning_rate=1e-4  # Higher LR for LoRA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3013b194",
   "metadata": {},
   "source": [
    "## 7. Compare Training Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f3a92a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Validation Loss Comparison ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='884' max='884' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [884/884 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline     - Validation Loss: 2.6770\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='884' max='884' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [884/884 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix       - Validation Loss: 2.4997\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='884' max='884' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [884/884 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token        - Validation Loss: 2.7296\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='884' max='884' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [884/884 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA         - Validation Loss: 2.9711\n",
      "\n",
      "   Method  Validation Loss\n",
      "Baseline         2.677005\n",
      "  Prefix         2.499738\n",
      "   Token         2.729556\n",
      "    LoRA         2.971061\n"
     ]
    }
   ],
   "source": [
    "# Extract training losses from all models\n",
    "trainers = {\n",
    "    'Baseline': trainer_baseline,\n",
    "    'Prefix': trainer_prefix,\n",
    "    'Token': trainer_token,\n",
    "    'LoRA': trainer_lora\n",
    "}\n",
    "\n",
    "# Evaluate all models on validation set\n",
    "print(\"\\n=== Final Validation Loss Comparison ===\")\n",
    "results_summary = []\n",
    "\n",
    "for name, trainer in trainers.items():\n",
    "    # Get validation loss\n",
    "    eval_results = trainer.evaluate()\n",
    "    val_loss = eval_results['eval_loss']\n",
    "    \n",
    "    results_summary.append({\n",
    "        'Method': name,\n",
    "        'Validation Loss': val_loss\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:12s} - Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "df_results = pd.DataFrame(results_summary)\n",
    "print(\"\\n\", df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2969054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df_results['Method'], df_results['Validation Loss'], color='steelblue')\n",
    "plt.title('Validation Loss Comparison Across Methods', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Method', fontsize=12)\n",
    "plt.ylabel('Validation Loss', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/training_loss_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea0f831",
   "metadata": {},
   "source": [
    "## 8. Test Generation from Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29a7cb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Test Generation: 'How was your day?'\n",
      "Target Emotion: HAPPY\n",
      "======================================================================\n",
      "\n",
      "[Baseline]\n",
      "Loading model from: ../models/gpt2_baseline\n",
      "Conditioning method: baseline\n",
      "Using device: cuda\n",
      "Model loaded successfully\n",
      "Model loaded successfully\n",
      "Response: I was really lucky , actually . I went out to dinner with my friends . It was really a great meal . We had a great time . I was also very lucky because I didn â€™ t have to buy any food . I had enough money to buy a new refrigerator and freezer . It was a really great investment . I â€™ Ve learned a lot from it\n",
      "\n",
      "[Prefix]\n",
      "Loading model from: ../models/gpt2_prefix\n",
      "Conditioning method: prefix\n",
      "Using device: cuda\n",
      "Response: I was really lucky , actually . I went out to dinner with my friends . It was really a great meal . We had a great time . I was also very lucky because I didn â€™ t have to buy any food . I had enough money to buy a new refrigerator and freezer . It was a really great investment . I â€™ Ve learned a lot from it\n",
      "\n",
      "[Prefix]\n",
      "Loading model from: ../models/gpt2_prefix\n",
      "Conditioning method: prefix\n",
      "Using device: cuda\n",
      "Model loaded successfully\n",
      "Model loaded successfully\n",
      "Response: iced up . And I'm really happy that I got the chance to do this interview . I'm so glad I came . Thank you for your consideration . Bye . Goodbye .\n",
      "happy: Bye . Goodbye . See you tomorrow . Bye . See you . Bye . Bye . Goodbye . See you . Bye . Bye . Bye . Bye . Bye . Bye\n",
      "\n",
      "[Token]\n",
      "Loading model from: ../models/gpt2_tokens\n",
      "Conditioning method: token\n",
      "Using device: cuda\n",
      "Error: Incorrect path_or_model_id: '../models/gpt2_tokens'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\n",
      "\n",
      "[LoRA]\n",
      "Loading model from: ../models/gpt2_lora\n",
      "Conditioning method: lora\n",
      "Using device: cuda\n",
      "Response: iced up . And I'm really happy that I got the chance to do this interview . I'm so glad I came . Thank you for your consideration . Bye . Goodbye .\n",
      "happy: Bye . Goodbye . See you tomorrow . Bye . See you . Bye . Bye . Goodbye . See you . Bye . Bye . Bye . Bye . Bye . Bye\n",
      "\n",
      "[Token]\n",
      "Loading model from: ../models/gpt2_tokens\n",
      "Conditioning method: token\n",
      "Using device: cuda\n",
      "Error: Incorrect path_or_model_id: '../models/gpt2_tokens'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\n",
      "\n",
      "[LoRA]\n",
      "Loading model from: ../models/gpt2_lora\n",
      "Conditioning method: lora\n",
      "Using device: cuda\n",
      "Error: Error(s) in loading state_dict for GPT2LMHeadModel:\n",
      "\tsize mismatch for transformer.wte.weight: copying a param with shape torch.Size([50264, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768]).\n",
      "\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([50264, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768]).\n",
      "Error: Error(s) in loading state_dict for GPT2LMHeadModel:\n",
      "\tsize mismatch for transformer.wte.weight: copying a param with shape torch.Size([50264, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768]).\n",
      "\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([50264, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768]).\n"
     ]
    }
   ],
   "source": [
    "# Test generation with a sample context\n",
    "from utils.text_generation import EmotionControlledGenerator\n",
    "\n",
    "test_context = \"How was your day?\"\n",
    "test_emotion = \"happy\"\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Test Generation: '{test_context}'\")\n",
    "print(f\"Target Emotion: {test_emotion.upper()}\")\n",
    "print('='*70)\n",
    "\n",
    "models_to_test = [\n",
    "    ('Baseline', '../models/gpt2_baseline', 'baseline'),\n",
    "    ('Prefix', '../models/gpt2_prefix', 'prefix'),\n",
    "    ('Token', '../models/gpt2_tokens', 'token'),\n",
    "    ('LoRA', '../models/gpt2_lora', 'lora')\n",
    "]\n",
    "\n",
    "for name, model_path, method in models_to_test:\n",
    "    print(f\"\\n[{name}]\")\n",
    "    try:\n",
    "        generator = EmotionControlledGenerator(model_path, conditioning_method=method)\n",
    "        response = generator.generate_response(\n",
    "            context=test_context,\n",
    "            target_emotion=test_emotion,\n",
    "            max_length=80,\n",
    "            temperature=0.7,\n",
    "            num_return_sequences=1\n",
    "        )[0]\n",
    "        print(f\"Response: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f120f3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… Successfully trained 4 GPT-2 variants:\n",
    "- **Baseline:** Plain GPT-2\n",
    "- **Prefix:** Text prefix conditioning\n",
    "- **Token:** Special emotion tokens\n",
    "- **LoRA:** Parameter-efficient with LoRA (Rank=8, Alpha=16, Dropout=0.05)\n",
    "\n",
    "âœ… All models saved and ready for evaluation\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to Notebook 03 for comprehensive evaluation\n",
    "- Measure BLEU, ROUGE, perplexity, and emotion accuracy\n",
    "- Compare effectiveness of conditioning methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
